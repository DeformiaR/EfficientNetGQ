{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ccd5853",
   "metadata": {},
   "source": [
    "# Load and Preprocess Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e8c84b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D , Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras import regularizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba80cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4199 images belonging to 3 classes.\n",
      "Found 900 images belonging to 3 classes.\n",
      "Found 900 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Data Augmentation for training to improve generalization\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,           # Random rotations\n",
    "    width_shift_range=0.2,       # Random horizontal shift\n",
    "    height_shift_range=0.2,      # Random vertical shift\n",
    "    shear_range=0.2,             # Shear transformation\n",
    "    zoom_range=0.2,              # Zoom in/out\n",
    "    horizontal_flip=True,        # Flip images horizontally\n",
    "    fill_mode='nearest'          # How to fill newly created pixels\n",
    ").flow_from_directory(\n",
    "    \"D:/Senior_WorkTable/GrowQuest_Data_Final/train\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Validation generator (no augmentation, just rescaling)\n",
    "val_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    \"D:/Senior_WorkTable/GrowQuest_Data_Final/val\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Test generator (no augmentation, shuffle=False)\n",
    "test_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    \"D:/Senior_WorkTable/GrowQuest_Data_Final/test\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Important to leave it as False during evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6861bcea",
   "metadata": {},
   "source": [
    "# Load and Fine-Tune EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4d58b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'Anthracnose': 2000 images\n",
      "Class 'Healthy_Leaf': 1999 images\n",
      "Class 'Pest_Damage': 2000 images\n"
     ]
    }
   ],
   "source": [
    "all_classes = np.concatenate([\n",
    "    train_gen.classes,\n",
    "    val_gen.classes,\n",
    "    test_gen.classes\n",
    "])\n",
    "\n",
    "# Count how many images per class\n",
    "class_counts = Counter(all_classes)\n",
    "\n",
    "# Get class label names\n",
    "class_labels = train_gen.class_indices\n",
    "idx_to_class = {v: k for k, v in class_labels.items()}\n",
    "\n",
    "# Print results\n",
    "for class_idx, count in class_counts.items():\n",
    "    print(f\"Class '{idx_to_class[class_idx]}': {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6459f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.3)(x)  # Add dropout layer\n",
    "x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = BatchNormalization()(x)                 \n",
    "x = Dropout(0.5)(x)  # Add dropout layer\n",
    "output = Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Freeze base model initially\n",
    "base_model.trainable = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ef5c61",
   "metadata": {},
   "source": [
    "# Fine-Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a42d05",
   "metadata": {},
   "source": [
    "Unfreeze base layers and fine-tune at lower LR:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201f13e",
   "metadata": {},
   "source": [
    "### add EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c89e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',     # Monitor validation loss\n",
    "    patience=3,             # Stop after 3 epochs with no improvement\n",
    "    restore_best_weights=True # Restore weights from the best epoch\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau (monitor='val_loss',\n",
    "                            factor=0.5,\n",
    "                            patience=2, \n",
    "                            min_lr=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "model.compile(optimizer=Adam(learning_rate=1e-3), \n",
    "            loss='categorical_crossentropy', \n",
    "            metrics=['accuracy', Precision(), Recall()],\n",
    ")\n",
    "\n",
    "history = model.fit(train_gen, \n",
    "                    validation_data=val_gen,\n",
    "                    epochs=10,\n",
    "                    callbacks=[early_stop,reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520487fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Define a function to build the model\n",
    "def build_model(hp):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Hyperparameters for tuning\n",
    "    dropout_rate_1 = hp.Float('dropout_rate_1', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    dropout_rate_2 = hp.Float('dropout_rate_2', min_value=0.3, max_value=0.6, step=0.1)\n",
    "    num_neurons_1 = hp.Int('num_neurons_1', min_value=64, max_value=512, step=64)\n",
    "    num_neurons_2 = hp.Int('num_neurons_2', min_value=64, max_value=512, step=64)\n",
    "    l2_reg = hp.Float('l2_reg', min_value=1e-5, max_value=1e-3, sampling='LOG')\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(dropout_rate_1)(x)  # Add dropout layer with hyperparameter\n",
    "    x = Dense(num_neurons_1, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(x)  # Add L2 regularization\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate_2)(x)  # Add second dropout layer\n",
    "    x = Dense(num_neurons_2, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    output = Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    \n",
    "    # Freeze the base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy', Precision(), Recall()]\n",
    "                )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize Keras Tuner with Random Search\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='hyperparameter_tuning'\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(train_gen, epochs=10, validation_data=val_gen)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(f'Best Hyperparameters: {best_hyperparameters.values}')\n",
    "\n",
    "# Optionally, you can train the best model on more epochs\n",
    "best_model.fit(train_gen, epochs=20, validation_data=val_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15020f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
